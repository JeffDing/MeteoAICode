{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344a0d47",
   "metadata": {
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 0 前言\n",
    "\n",
    "## 0.1 项目背景\n",
    "近年来，随着中国经济的不断发展和城市化步伐的加快，大量的土地被不断开发和征用，导致原有的土地覆盖发生了很大的变化，城市土地利用的高动态变化对决策者和管理者管理新城市社区提出了挑战。因此，如何准确识别城市土地利用覆盖，以便为土地利用规划、管理和生态修复提供科学有效的参考，有着十分重要的意义。科学规划国土空间，需要对土地利用现状和城市功能区空间分布情况进行深入分析，如何准确有效地识别城市土地利用类型也成为关键问题。\n",
    "\n",
    "遥感作为一种可以快速获取地表覆盖信息的技术，具有快速、宏观、综合、准确、周期性、低成本等优势，已经成为城市用地信息提取与监测的重要手段 。但遥感影像中提取的特征仅仅描述了地物的自然属性，无法完全对接社会经济属性明显的城市土地利用类型。\n",
    "\n",
    "随着互联网的快速发展，大量基于位置服务的社交媒体数据为城市土地利用分类和城市空间结构分析研究提供了丰富的数据源。社交媒体数据能够反映人类经济社会活动的内在特征，补充了高分辨率遥感影像数据无法描述的地物内部经济社会属性，有助于城市土地利用分类。例如，**兴趣点（Points-of-interest，简称POIs）数据为城市土地利用分类研究提供了大量的语义信息，包括名称、地址、功能、经纬度等，可以描述地理空间中各类商业性设施和社会服务性设施，蕴含着丰富的人文经济特征**，补充了遥感影像所缺乏的语义信息，是城市空间分析中的重要基础地理数据之一。已有研究表明，**POI 的分布特征可以有效地说明地块的功能**。因此，**研究 POI 数据支持下的城市土地利用分类，可以有效挖掘社交媒体数据中的社会经济信息，显著提高识别城市土地利用类型的精度。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccc7c2",
   "metadata": {
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "624ef709c47d0200184a3cb9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 0.2 任务简介\n",
    "本期，我们将基于和鲸 ModelWhale 平台，手把手教大家动手学习如何利用兴趣点数据进行土地利用分类，在这里我们将向大家演示POI数据的爬取、处理、训练以及聚类可视化分析等一套完整的基本流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db1097",
   "metadata": {
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "jupyter": {},
    "mdEditEnable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 1 兴趣点爬虫和实现（API）\n",
    "**本节使用镜像为Python 3.7 TensorFlow 2.6 PyTorch 1.8，使用的计算资源是2核8G CPU资源，Kernel类型为Python3。由于使用的镜像很基础，因此大家使用不包含torch库的镜像都可以。但爬取poi数据不涉及GPU的使用，因此大家使用CPU资源就可以了。**\n",
    "\n",
    "\n",
    " POI 是“ Point of Interest ”的缩写，中文可以翻译为“兴趣点”，其主要包含名称、地址、功能、经纬度等属性。在地理信息系统中，一个 POI 可以是一栋房子、一个商铺、一个邮筒、一个公交站等。POI数据分类众多，包括美食、购物、旅游景点、政府机构、交通设施等地理信息数据。利用兴趣点（POI）进行土地利用分类的前提是要先获取兴趣点数据。那么如何自动便捷地获取兴趣点数据呢？\n",
    "\t\t\t\t\n",
    "传统的地理信息采集方法需要地图测绘人员采用精密的测绘仪器去获取一个兴趣点的经纬度，然后再标记下来，是一个非常费时费事的工作，而利用 Python 可以方便的批量调用 API 即 Application Programming Interface 应用程序接口，抓取数据、返回兴趣点的名称、经纬度坐标等数据。\n",
    "\t\t\t\t\n",
    "本节利用百度地图 API ，实现批量抓取美食类 POI 数据。其他类型 POI 数据以此类推，以深圳市为例。那如何简单地爬取兴趣点数据呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88209ef",
   "metadata": {
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "624ef709c47d0200184a3cb9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.1 任务准备\n",
    "登录百度账号，在百度地图开发者平台的API控制台申请一个服务端的ak，主要用到的是Place API。去百度地图开放平台申请密匙：http://lbsyun.baidu.com/\n",
    "\t\t\t\t\n",
    "步骤：登录----->控制台----->注册登录成为开发者（已成为则跳过该步骤）----->创建应用------>填写应用名称----->应用类型选择浏览器端---->refer白名单输入“*”—>创建应用成功----->得到密匙（AK）\n",
    "\n",
    "检校方式可设置成IP白名单，IP直接设置成了*表示容许所有的referer。\n",
    "\n",
    "\n",
    "创建成功后就可以在”我的应用“中看到自己创建的ak了，把它复制下来。那么ak该如何使用呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44298e",
   "metadata": {
    "id": "4FC61B519BE24DD590C9913A15FBDA95",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.2 爬取POI数据\n",
    "### 1.2.1 确定url解析地址\n",
    "在使用ak之前，首先我们要导入程序所需要的库，构建所要抓取的根 url ，根据百度开放平台中的服务文档的说明确定 url 的内容，可点击[百度服务平台地点检索服务介绍](https://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-placeapi)查询。接下来我们要确定存放解析地址出来的坐标文件的工作目录。 在本例中，我们使用圆形区域进行检索，这样我们就需要先确定检索区域范围中一些圆心点的坐标。选取圆心点和半径大小时可根据获取数据的需求，覆盖到感兴趣的所有区域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbeb7c1",
   "metadata": {
    "collapsed": false,
    "id": "437E283544AA457BBD7BBB916DFFEA5C",
    "jupyter": {
     "outputs_hidden": false
    },
    "mdEditEnable": false,
    "notebookId": "62eb7c2fdf5dbc7d470c58d9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"首先导入相关的库及一些初始的文件路径\"\"\"\n",
    "from urllib.request import urlopen #调用urllib.request库里面的urlopen命令，向网页发送请求并打开网页\n",
    "import re, os\n",
    "import csv\n",
    "import requests\n",
    "import json #调用库，json是一种便于解析的格式\n",
    "import time #调用库，time是时间，防止访问次数过于频繁被系统后台禁止\n",
    "from urllib.parse import quote #调用urllib.parse库里面的查询命令\n",
    "\n",
    "# 构建抓取的根URL\n",
    "baseURL = 'http://api.map.baidu.com/place/v2/search?output=json&'#抓取网页的地址\n",
    "ak = 'Tny0Gj8oz6LYvwguFDwoOEPbMFwrEU5v' # 上一步在百度地图中申请的ak，这里用自己申请的ak，每个ak每天都有访问限制\n",
    "query = quote('美食') #爬取的POI类型是美食，quote是把中文转化成url的格式\n",
    "scope = '2'#返回内容的详细程度，2是比较详尽的意思\n",
    "\n",
    "# 确定工作目录\n",
    "input_path = u'/home/mw/input/Data5799/数据集'  # 爬取坐标文件所在的路径\n",
    "output_path = u'/home/mw/project/'  # 输出文件路径\n",
    "outputFile = output_path + 'BaiduPOI_shenzhen.txt'#定义输出的POI文件，BaiduPOI_shenzhen自行命名\n",
    "poiList = [] #创建一个poi列表存储爬出来的数据\n",
    "# poiList.append(['name','lng','lat','type','province','city','area']) # 添加表头，包含poi点的名称、经纬度、所属类别、所属省、市、区县等信息\n",
    "poiList.append('name'+','+'lng'+','+'lat'+','+'type'+','+'province'+','+'city'+','+'area') # 添加表头，包含poi点的名称、经纬度、所属类别、所属省、市、区县等信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83a910",
   "metadata": {
    "id": "4076690279414860A5473FBCE1200FD4",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62eb7c2fdf5dbc7d470c58d9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.2.2 利用url链接抓取poi数据并解析\n",
    "之后我们定义一个fetch小程序用于抓取 url 链接，接下来读取检索点(圆心点)的坐标文件，生成抓取列表。循环所生成的 coordinates 列表，对列表中的每一个coordinate 进行 fetch ，并存储所得数据 。\n",
    "注：爬取poi时坐标默认为百度地图的坐标类型 BD09II ，否则使用其他坐标系统时(例如 WGS84 )，检索点会有偏移，我们的数据坐标系是 WGS84，因此需要先进行由 WGS84坐标系到百度坐标系的转换。调用百度地图坐标转换的 API 即可实现由非百度坐标转为百度坐标，设置转换前的坐标类型请参考百度开放平台中[坐标类型说明](https://lbsyun.baidu.com/index.php?title=webapi/guide/changeposition)。\n",
    "\n",
    "下一步进行实际的抓取动作，根据 BaseURL ，构建抓取所用 URL，生成抓取结果并将结果赋给 response ， 同时确认此次调用返回数据的页数。\n",
    "\n",
    "我们在编写程序之前，需要手动单次进行 POI 信息抓取，访问相应 url 后，我们发现 results 的格式是列表，因此我们在编写程序时，要先对列表里的字典进行提取，再提取字典中的信息。具体来说，对应每个坐标点，循环提取每页抓取到的 poi 信息后，将提取到的结果赋给 contents 。 嵌套在上一个循环内，开始循环 contents 列表中的所需信息，对 contents 列表中的信息进行进一步提取，即 poi 的名字、经纬度和 uid。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edca745",
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "id": "16BEDEF83DF24B65ABB0E52615CECDCB",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前处理到了第1/10个点\n",
      "wgs_x:114.0367892,wgs_y:22.57155172\n",
      "lng:114.04842451721946,lat:22.574598269509004\n",
      "6 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "Start to fetch page 6\n",
      "当前处理到了第2/10个点\n",
      "wgs_x:114.1884508,wgs_y:22.64307492\n",
      "lng:114.19992695686557,lat:22.646024835480397\n",
      "6 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "Start to fetch page 6\n",
      "当前处理到了第3/10个点\n",
      "wgs_x:113.9311081,wgs_y:22.49366838\n",
      "lng:113.94254960937353,lat:22.49630424510138\n",
      "5 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "当前处理到了第4/10个点\n",
      "wgs_x:114.24503390000001,wgs_y:22.75549651\n",
      "lng:114.25630678592624,lat:22.758973190106225\n",
      "5 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "当前处理到了第5/10个点\n",
      "wgs_x:113.840994,wgs_y:22.76784084\n",
      "lng:113.8524870638321,lat:22.770829034484752\n",
      "5 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "当前处理到了第6/10个点\n",
      "wgs_x:113.84098090000002,wgs_y:22.74052795\n",
      "lng:113.85249330229394,lat:22.743511811284954\n",
      "4 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "当前处理到了第7/10个点\n",
      "wgs_x:114.2450018,wgs_y:22.72840205\n",
      "lng:114.25629869166909,lat:22.731875976169686\n",
      "5 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "当前处理到了第8/10个点\n",
      "wgs_x:114.2449955,wgs_y:22.71500474\n",
      "lng:114.25629770398348,lat:22.71847608803892\n",
      "3 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "当前处理到了第9/10个点\n",
      "wgs_x:114.21995249999999,wgs_y:22.70099247\n",
      "lng:114.23129492312178,lat:22.70441710070237\n",
      "5 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "当前处理到了第10/10个点\n",
      "wgs_x:114.2199923,wgs_y:22.67622399\n",
      "lng:114.23131050690853,lat:22.67963879088245\n",
      "5 in Total\n",
      "Start to fetch page 0\n",
      "Start to fetch page 1\n",
      "Start to fetch page 2\n",
      "Start to fetch page 3\n",
      "Start to fetch page 4\n",
      "Start to fetch page 5\n",
      "poi爬取完毕！\n"
     ]
    }
   ],
   "source": [
    "# 定义抓取动作\n",
    "# 分四步：访问-读取-解析-休眠\n",
    "def fetch(url):#定义抓取的参数是url\n",
    "    feedback = requests.get(url)\n",
    "    feedback = feedback.content\n",
    "    data = feedback.decode(\"utf8\") # 读取url\n",
    "    response = json.loads(data) # 对返回结果进行解析,将已编码的 JSON 字符串解码为 Python 对象，就是python解码json对象\n",
    "    time.sleep(2) # 暂停2秒，防止访问次数过于频繁被系统后台禁止return response #返回抓取结果\n",
    "    return response\n",
    "\n",
    "# 官方转换函数\n",
    "# 在调用百度 API 进行 POI 爬取时，由于默认输入坐标类型属于百度坐标系（BD09ll），而我们第一手数据往往并不是百度坐标。\n",
    "# 所以，我们需要将WGS84坐标系转换成百度地图中使用的坐标，才可将转化后的坐标在百度地图JavaScriptAPI、静态图 API、Web 服务 API 等产品中使用。\n",
    "def wgs2bd0911(wgs_x, wgs_y):\n",
    "    # from:1是从WGS84坐标系转换 to:5是转为bd0911\n",
    "    url = 'http://api.map.baidu.com/geoconv/v1/?coords={}+&from=1&to=5&output=json&ak={}'.format(str(wgs_x) + ',' + str(wgs_y), ak)\n",
    "    response = requests.get(url)\n",
    "    response = response.content\n",
    "    res = response.decode(\"utf8\") \n",
    "    temp = json.loads(res)  # 对返回结果进行解析\n",
    "    bd09mc_x = 0\n",
    "    bd09mc_y = 0\n",
    "    if temp['status'] == 0:\n",
    "        bd09mc_x = temp['result'][0]['x']  # 转换后的经度\n",
    "        bd09mc_y = temp['result'][0]['y']  # 转换后的纬度\n",
    "\n",
    "    return bd09mc_x, bd09mc_y\n",
    "\n",
    "# 定义读csv文件的函数\n",
    "def read_csv(filepath):\n",
    "    data = []\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, mode='r', encoding='gb18030') as f:\n",
    "            lines = csv.reader(f)  # 此处读取到的数据是将每行数据当做列表返回的\n",
    "            for line in lines:\n",
    "                data.append(line)\n",
    "        return data\n",
    "    else:\n",
    "        print('filepath is wrong：{}'.format(filepath))\n",
    "        return []\n",
    "\n",
    "# 读取坐标文件，生成抓取队列\n",
    "data = read_csv(input_path + '/'+ 'poi_integrate_id.csv')  #打开坐标文件即经纬度坐标文件，r是读取的意思\n",
    "header = data[0]# 记录 header\n",
    "data = data[1:]# 去掉 header\n",
    "# 循环coordinates，对每一个coordinates进行fetch，并存储所得数据\n",
    "n = 10  # 定义要中心坐标点的个数，此处作为示例，仅挑选前10个中心坐标点爬取\n",
    "for i in range(n):#循环coordinates中的每一个坐标 \n",
    "    print('当前处理到了第{0}/{1}个点'.format(i + 1,n))\n",
    "    wgs_x, wgs_y = data[i][1], data[i][2]  # 提取坐标，0对应坐标文件中的第1列，1对应坐标文件中的第2列，2对应坐标文件中的第3列\n",
    "    print('wgs_x:{},wgs_y:{}'.format(wgs_x, wgs_y))  # 原始的wgs84坐标系坐标\n",
    "    lng, lat = wgs2bd0911(wgs_x, wgs_y)  # lng为BD09ll坐标系下的经度，lat为BD09ll坐标系下的纬度\n",
    "    print('lng:{},lat:{}'.format(lng, lat))  # 转换后的百度坐标系坐标\n",
    "    # 抓取动作\n",
    "    # 根据BaseURL，生成抓取所用URL\n",
    "    initialURL = baseURL + 'ak=' + ak + '&query=' + query + '&scope=' + scope + \\\n",
    "                '&location=' + str(lat) + ',' + str(lng) + '&radius=800' + '&page_size=20&page_num=0'#ak,query,scope已经介绍过，不再赘述，location是坐标即经纬度，radius=800即抓取半径是800米,&page_size=20&page_num=0即每页返回20条数据，超过20条数据进行翻页\n",
    "    # 确认此次调用返回数据的页数\n",
    "    response = fetch(initialURL) # 返回抓取结果\n",
    "    # print(response)\n",
    "    totalNum = response['total'] # 返回抓取结果数据的数量\n",
    "    numPages = int(totalNum/20)+1 # 计算页数取整后+1\n",
    "    print (str(numPages) + ' in Total') # 在屏幕上打印页数\n",
    "    # 开始翻页\n",
    "    for i in range(0, numPages+1): #循环每页\n",
    "        print ('Start to fetch page ' + str(i)) #在屏幕上打印抓取到第几页\n",
    "        URL = baseURL + 'ak=' + ak + '&query=' + query + '&scope=' + scope + '&location=' + str(lat) + ',' + str(lng) + '&radius=800' + '&page_size=20&page_num=' + str(i)\n",
    "        \n",
    "        response = fetch(URL) #返回抓取url的结果\n",
    "        contents = response['results'] #将返回的结果放在列表contents中\n",
    "        # print(contents) # contents是列表的格式\n",
    "        # 开始循环抓取列表中的所需信息\n",
    "        for content in contents: #content是列表中的内容，是字典格式\n",
    "            # print(content)\n",
    "            name = content['name'] #由于抓取到的数据很多，我们进行进一步的提取；提取poi名字\n",
    "            lat = content['location']['lat'] # 提取纬度\n",
    "            lng = content['location']['lng'] # 提取经度\n",
    "            catagory = content['detail_info']['type'] # 所属分类，如’hotel’、’cater’\n",
    "            province = content['province'] # 所属省份\n",
    "            city = content['city'] # 所属城市\n",
    "            area = content['area'] # 所属区县\n",
    "            \n",
    "            # 定义输出的结果为名字+经度+纬度+类别+省+市+区县\n",
    "            poiInfo = name+','+str(lng)+','+str(lat)+','+catagory+','+province+','+city+','+area     \n",
    "            poiList.append(poiInfo) # 提取一个poiInfo加到poiList列表中        \n",
    "            # poiList.append([name,str(lng),str(lat),catagory,province,city,area]) # 提取一个poiInfo加到poiList列表中\n",
    "\n",
    "# 最后生成一个csv文件，输出所有抓去结果\n",
    "# with open(outputFile, mode='w', encoding='gb18030') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     for i in poiList:\n",
    "#         writer.writerow(i)\n",
    "\n",
    "# 最后生成一个 txt 文件，输出所有抓取结果。\n",
    "with open(outputFile, 'w') as f:\n",
    "    for poiInfo in poiList:\n",
    "        f.write(poiInfo + '\\n')\n",
    "        f.close()  # 出于效率的考虑，只有使用close()函数关闭文件时，才会将缓冲区中的数据真正写入文件中\n",
    "        f = open(outputFile, 'a')\n",
    "print('poi爬取完毕！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc1959",
   "metadata": {
    "hide_input": false,
    "id": "D672A6DA22044421924584DCEFE45A4F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.3 小结\n",
    "本节主要通过构建一个fetch小程序抓取网页中的 url 链接，并对输入的坐标文件中的每一个坐标进行循环、提取目标参数，最终生成 poi 数据文件，完成整个调用 API 爬取百度 POI 数据的过程。\n",
    "\n",
    "本节我们主要学习到了这些知识：\n",
    "1. 百度地图开发者平台API的调用；\n",
    "2.  基于百度地图爬取poi数据的整个基本流程以及poi数据的坐标转换。\n",
    "\n",
    "在百度地图开放平台中，我们可以获得美食、购物、旅游景点、交通设施等各类 POI 数据。每个类型的 POI 数据对应着一个关键词。例如，我们想爬取公司企业类的 poi 数据，则只需要在上述程序中，将 query = quote ('美食') 中‘美食’改成‘公司企业’即可。其他类型的 POI 数据以此类推。每个类型的关键词在百度开放平台网页中有具体说明，可点击 [百度地图开放平台 poi 分类](https://lbsyun.baidu.com/index.php?title=lbscloud/poitags)进行查看。\n",
    "\n",
    "下一节，我们将学习Word2Vec向量化模型的有关知识，以及如何基于爬取的这些poi数据利用向量化模型训练出具体的poi向量，以便为后续求地块的向量及聚类打好基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32ce80",
   "metadata": {
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 2 兴趣点的处理与向量训练\n",
    "\n",
    "上一节说明了兴趣点的自动获取方式，接下来介绍利用所获取的兴趣点进行城市土地利用分类的方法。在此之前，需要先将兴趣点数据训练成向量，再利用各个地块内的平均兴趣点向量来对地块进行表征，最后利用各个地块的表征向量完成土地利用分类。poi_integrate_id.csv文件中“ZoneID”字段存储了每个兴趣点所属地块编号。\n",
    "\n",
    "如前所述，本节大家需要注意：\n",
    "1. 如何定义兴趣点向量的训练模型和基本参数？\n",
    "2. 如何对poi数据和地块数据进行预处理以及进行poi数据的训练？\n",
    "\n",
    "**本节使用镜像为Python 3.7 TensorFlow 2.6 PyTorch 1.8，使用的计算资源是2核8G CPU资源，Kernel类型为Python3。特别注意的是，由于后续需要进行poi的投影坐标转换和距离计算，因此需要额外安装pyproj库，版本为2.4.0（本节后面直接在notebook中用pip导入），如果想一劳永逸，可以先自行在自己的镜像导入pyproj库（自己的镜像务必包含torch库）。此外，训练poi数据可以用CPU也可以用GPU，由于本案例训练的模型不复杂，训练的epoch也不多，因此用CPU也不慢。如果大家设置较大的epoch值，推荐使用GPU训练。**\n",
    "\n",
    "**特别注意，由于后面要在程序执行过程中导入pyproj库，因此建议大家逐cell运行，安装好pyproj库后重启核再一键重新运行即可。**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf5d1b",
   "metadata": {
    "id": "BD9939525E8C4CE48B74EB7E074309F7",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62d909318d38c00d5dfdf15f",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Word2Vec词向量训练模型简介\n",
    "将兴趣点转化成向量的方法有很多，例如经典的TF-IDF算法、Woed2Vec算法等，本项目以Word2Vec作为基础演示模型。\n",
    "\n",
    "Word2Vec的方法是在2013年的论文《Efficient Estimation of Word Representations inVector Space》中提出的，作者来自google，文章下载链接：https://arxiv.org/pdf/1301.3781.pdf\n",
    "\n",
    "Word2Vec是一种将单词转为向量的方法，其包含两种算法，分别是Skip-gram和CBOW，它们的最大区别是Skip-gram是通过中心词去预测中心词周围的词，而CBOW是通过周围的词去预测中心词。\n",
    "\n",
    "### 2.1.1 独热编码简介\n",
    "词向量顾名思义就是每个单词可以用唯一的一个向量表征，词向量维度大小为整个词汇表的大小。对于每个具体的词汇表中的词，将它们随机排序后，对应的位置置为1，这种词向量的编码方式我们一般叫做独热编码。\n",
    "\n",
    "比如下面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。\n",
    "杭州 [0,0,0,0,0,0,0,1,0,……0,0,0,0,0,0,0]\n",
    "上海 [0,0,0,0,1,0,0,0,0,……0,0,0,0,0,0,0]\n",
    "宁波 [0,0,0,1,0,0,0,0,0,……0,0,0,0,0,0,0]\n",
    "北京 [0,0,0,0,0,0,0,0,0,……1,0,0,0,0,0,0]\n",
    "\n",
    "可以看到，独热编码用来表示词向量非常简单，但是却有很多问题。最大的问题是我们的词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示简直是内存的灾难。这样的向量其实除了一个位置是1，其余的位置全部都是0，表达的效率不高，能不能把词向量的维度变小呢？\n",
    "\n",
    "分布式表征方法可以解决独热编码存在的问题，它的思路是通过训练，将每个词都映射到一个较短（维度较低）的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。一般我们取8的倍数，例如64/128/256维。\n",
    "\n",
    "### 2.1.2 Word2Vec模型概述\n",
    "Word2Vec模型是一种分布式表征方法，它可以将独热编码的向量转化为低维度的连续值，也就是稠密向量，稠密向量中的数字不仅只有0和1，而是有一些其他的浮点数，例如0.9、0.8等。并且其中意思相近的词将被映射到向量空间中相近的位置。如果将embed后的城市向量通过PCA降维后可视化展示出来，那就是这个样子：\n",
    "\n",
    "![Image Name](./pic/rggbmss1as.png)\n",
    "\n",
    "可以明显看到，北京、上海和东京的向量的距离较近，因为它们都位于亚洲。而纽约和华盛顿的词向量距离较近，因为它们都位于美国。而北京、上海和东京与纽约和华盛顿相隔较远，是因为它们分别属于不同的洲际。**因此，经过Word2Vec表征后的单词，在空间中的距离远近就体现了它们的语义相似性。距离越近，它们的语义就越相似。**那Word2Vec的结构是什么样的呢？它复杂吗？\n",
    "\n",
    "Word2Vec模型其实就是简单化的神经网络。一共有三层：输入层、隐藏层（也叫投影层）和输出层。输入是独热编码后的向量，隐藏层没有激活函数，也就是线性的单元。输出层维度跟输入层的维度一样，用的是Softmax回归。**我们要获取的训练好的词向量其实就是隐藏层的输出单元，有的地方定为输入层和隐藏层之间的权重**，其实说的是一回事。\n",
    "\n",
    "Word2Vec模型分为CBOW(Continuous Bag-of-Words)模型和Skip-gram模型。CBOW模型根据某个中心词前后n个连续的词，来计算该中心词出现的概率，即用上下文预测目标词。Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文。两种模型结构简易示意图如下：\n",
    "\n",
    "\n",
    "![Image Name](./pic/rgge65swmv.png)\n",
    "\n",
    "本例选择Skip-gram模型训练兴趣点向量，因此此处重点介绍Skip-gram模型。我们将训练神经网络做以下工作：给定一个句子中间的一个指定单词（输入单词），查看附近的单词并随机选择一个。网络将告诉我们，词汇中每个单词是我们选择的\"附近单词\"的概率。“附近”指的是算法中的一个“窗口大小”参数，典型的窗口大小可能是5，意思是后面5个字，前面5个字（总共10个）。\n",
    "\n",
    "输出概率将与在输入单词附近找到词汇表中每个单词的可能性有关。例如，如果我们把单词“Soviet”作为训练后模型的输入，那么单词，如“Union”和“Russia”，的概率将远远高于不相关单词，如“watermelon”和“kangaroo”。\n",
    "\n",
    "我们将通过给神经网络提供从训练文档中找到的单词对来训练神经网络。下面的例子展示了一些来自句子“The quick brown fox jumps over the lazy dog”的训练样本（单词对）。在这个例子中，我使用了一个较小的窗口尺寸（2），蓝色高亮的单词是输入单词，右边的Training Samples表示输入网络的训练词对（pairs）。\n",
    "\n",
    "\n",
    "![Image Name](./pic/rggfpzdqjs.png)\n",
    "\n",
    "这个网络将要从每个对出现的次数中学习统计量。因此，例如，这个网络可能会得到比(“Soviet”, “Sasquatch”)更多的训练样本(“Soviet”, “Union”)。当训练结束后，如果我们将单词“Soviet”作为输入，随后它将给“Union”和“Russia”输出比“Sasquatch”更高的概率。\n",
    "\n",
    "以上是Word2Vec训练单词的模型和方法，而兴趣点和单词有所不同，兴趣点是空间实体，具有位置信息（经纬度），而单词则简单一些。那如何将单词层面的Word2Vec方法，迁移到空间层面的兴趣点方面呢？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.3 由Word2Vec模型到Place2Vec模型\n",
    "为了演示方便，本例最终以Place2Vec模型作为兴趣点向量训练模型。更多关于Place2Vec模型，可以参照其原始论文：[From ITDL to Place2Vec – Reasoning About Place Type Similarity and Relatedness by Learning Embeddings From Augmented Spatial Contexts](https://www.researchgate.net/publication/320778966_From_ITDL_to_Place2Vec_--_Reasoning_About_Place_Type_Similarity_and_Relatedness_by_Learning_Embeddings_From_Augmented_Spatial_Contexts)。\n",
    "\n",
    "Place2Vec模型与Skip-gram模型区别不大，简而言之，Place2Vec模型是基于Skip-gram模型的。**他们最主要的区别是，Skip-gram模型的输入是根据句子中词语的顺序来构造pairs的，而Place2Vec模型则是根据兴趣点在空间中的距离来构造pairs的。例如，假设某中心词为某餐饮店，取与它在空间中距离最近的10个兴趣点（数量可以随意定）作为周围词来构造兴趣点pairs，那么一共就可以构造10对pairs作为输入。**那么这样利用Place2Vec模型便可以将Skip-gram模型由单词层面迁移到兴趣点层面。\n",
    "\n",
    "**为了便于说明，后文\"利用Word2Vec模型训练的向量\"和\"利用Place2Vec模型训练的向量\"指代的是同一个意思，因为Place2Vec模型本来就是在Word2Vec模型的基础上改进得到的，本质还是Word2Vec模型模型。** \n",
    "\n",
    "下面是具体的利用Place2Vec模型训练兴趣点向量的方法："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c784b",
   "metadata": {
    "id": "56C499227C7B47CDAAB5A226038F2EEE",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62d909318d38c00d5dfdf15f",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.2 定义Skip-gram模型函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85b719",
   "metadata": {
    "id": "9CC2C00730D84060B05B2CB3DC612C01",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62d909318d38c00d5dfdf15f",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "在定义函数之前，由于此镜像不存在pyproj包，因此需要先下载一个pyproj包，用于后续的距离计算及投影坐标转换。**执行pip安装后需要重启核并重新运行才能生效**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7493347",
   "metadata": {
    "collapsed": false,
    "id": "6E67770D9BED4FCAB9E5221D7285C8C2",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62d909318d38c00d5dfdf15f",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.douban.com/simple/\n",
      "Requirement already satisfied: pyproj==2.4.0 in /opt/conda/lib/python3.7/site-packages (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyproj==2.4.0 -i https://pypi.douban.com/simple/  # pip安装，从指定镜像下载安装工具包，镜像URL可自行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bc76b8",
   "metadata": {
    "collapsed": false,
    "hide_input": false,
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "624ef709c47d0200184a3cb9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"导入相关的包\"\"\"\n",
    "import numpy as np\n",
    "import torch.nn as nn  # torch核心模块，包括常用神经网络及损失函数等，方便搭建模型\n",
    "import torch.optim as optim  # 该模块定义了不同梯度下降优化器方法\n",
    "import torch.utils.data as Data  # torch.utils是辅助模型训练、测试和结构优化的模块\n",
    "from scipy.spatial.distance import cdist  # 该函数用于计算两个输入集合的距离\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud  # torch.utils.data引入数据集（Dataset）和数据载入器（DataLoader）方便对训练中数据进行操作\n",
    "from collections import Counter  # Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary\n",
    "import random\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Transformer  # CRS初始坐标系，Transformer进行坐标转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889b0932",
   "metadata": {
    "collapsed": false,
    "id": "4FC61B519BE24DD590C9913A15FBDA95",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义Word2Vec模型的Skip-grim方法的函数\n",
    "# pairs为点对\n",
    "def skip_gram(vocabulary, word_pairs, word_to_idx, id_to_word, k):\n",
    "    \"\"\"\n",
    "    学习词向量的概念\n",
    "    用Skip-thought模型训练词向量\n",
    "    学习使用PyTorch dataset和dataloader\n",
    "    学习定义PyTorch模型\n",
    "    学习torch.nn中常见的Module\n",
    "    Embedding\n",
    "    学习常见的PyTorch operations\n",
    "    bmm\n",
    "    logsigmoid\n",
    "    保存和读取PyTorch模型\n",
    "    在这一份notebook中，我们会（尽可能）尝试复现论文Distributed Representations of Words and\n",
    "    Phrases and their Compositionality中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "    这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。\n",
    "    我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "    以下是一些我们没有实现的细节\n",
    "    subsampling：参考论文section 2.3\n",
    "    \"\"\"\n",
    "\n",
    "    # 设备选择\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "    # 为了保证实验结果的可以复现，我们经常会把各种random的seed固定在某一个值\n",
    "    random.seed(53113)\n",
    "    np.random.seed(53113)\n",
    "    torch.manual_seed(53113)\n",
    "    if USE_CUDA:\n",
    "        torch.cuda.manual_seed(53113)\n",
    "\n",
    "    # 设定一些超参数\n",
    "    K = 100  # number of negative samples 负采样单词数量\n",
    "    NUM_EPOCHS = 3  # 训练周期。模型将使用的训练集全部样本训练一遍即为一个epoch，设置3即模型将训练集重复训练3次\n",
    "    BATCH_SIZE = 128  # 批大小。小批量梯度下降算法中，设置的一次载入内存进行批梯度计算的样本数量\n",
    "    LEARNING_RATE = 0.2  # 学习率。梯度下降算法的步长或者收敛速度。过小收敛缓慢，过大则可能无法收敛，或者在最值附近震荡\n",
    "    EMBEDDING_SIZE = 256  # 训练的向量的维度\n",
    "\n",
    "    # 日志文件\n",
    "    LOG_FILE = 'word-embedding.log'\n",
    "\n",
    "    word_counts = np.array([count for count in vocabulary.values()], dtype=np.float32)  # 每个单词出现次数的列表\n",
    "\n",
    "    \"\"\"\n",
    "    单词出现频率  单词出现次数/单词总的个数\n",
    "    把频次乘以3/4效果会好一些  这是论文里面的trick  \n",
    "    word_freqs 用来做负采样 \n",
    "    \"\"\"\n",
    "    word_freqs = word_counts / np.sum(word_counts)\n",
    "    word_freqs = word_freqs ** (3. / 4.)\n",
    "    word_freqs = word_freqs / np.sum(word_freqs)\n",
    "    # print(word_freqs)\n",
    "\n",
    "    VOCAB_SIZE = len(id_to_word)\n",
    "\n",
    "    \"\"\"\n",
    "    实现Dataloader\n",
    "    一个dataloader需要以下内容：\n",
    "    把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "    保存vocabulary，单词count，normalized word frequency\n",
    "    每个iteration sample一个中心词\n",
    "    根据当前的中心词返回context单词\n",
    "    根据中心词sample一些negative单词\n",
    "    返回单词的counts\n",
    "    这里有一个好的tutorial介绍如何使用PyTorch dataloader. 为了使用dataloader，我们需要定义以下两个function:\n",
    "    __len__ function需要返回整个数据集中有多少个item\n",
    "    __get__ 根据给定的index返回一个item\n",
    "    有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。\n",
    "    \"\"\"\n",
    "\n",
    "    class WordEmbeddingDataset(tud.Dataset):\n",
    "        \"\"\"\n",
    "        text  单词文本  来自与train 训练数据\n",
    "        word_to_idx 单词到索引的映射  字典\n",
    "        idx_to_word 索引到单词的列表  下标即是索引\n",
    "        word_freqs 每个单词的频率  出现次数/总次数 ××（3/4）列表\n",
    "        word_counts 每个单词的出现次数\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, pair, word2idx, idx2word, freqs, counts):\n",
    "            super(WordEmbeddingDataset, self).__init__()\n",
    "            \"\"\"\n",
    "            将text转为数字索引编码的列表 \n",
    "            并转为tensor 张量\n",
    "            \"\"\"\n",
    "            self.text_encoded = pair  # 已经得到的（中心词，周围词）点对\n",
    "            self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "            self.word_to_idx = word2idx\n",
    "            self.idx_to_word = idx2word\n",
    "            self.word_freqs = torch.Tensor(freqs)\n",
    "            self.word_counts = torch.Tensor(counts)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.text_encoded)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            \"\"\"\n",
    "            首先 skgram 这个模型的任务就是  根据中心词 预测周围的词\n",
    "            这个function 返回一下数据item 用于训练\n",
    "            ---中心词  当前的词\n",
    "            ---这个单词附近的（上下文正确的单词)的单词\n",
    "            ---随机采样的K个单词作为负样本  就是负样本采样\n",
    "            \"\"\"\n",
    "            # ----中心词\n",
    "            center_word = self.text_encoded[idx][0]\n",
    "            # ---中心词窗口的附近正确的单词,共有k个\n",
    "            pos_words = self.text_encoded[idx][1:]\n",
    "            \"\"\"\n",
    "            负采样的单词  以及负采样的规则 \n",
    "            word_freqs 每个单词的频率  出现次数/总次数 ××（3/4）列表\n",
    "            word_freqs 用来做负采样     \n",
    "            torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor\n",
    "            作用是对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标。\n",
    "            输入是一个input张量，一个取样数量，和一个布尔值replacement。\n",
    "            input张量可以看成一个权重张量，每一个元素代表其在该行中的权重。如果有元素为0，那么在其他不为0的元素\n",
    "            被取干净之前，这个元素是不会被取到的。\n",
    "            n_samples是每一行的取值次数，该值不能大于每一样的元素数，否则会报错。\n",
    "            replacement指的是取样时是否是有放回的取样，True是有放回，False无放回。\n",
    "\n",
    "            按权重 取得是对应的下标  在这里就是按着每个单词的频率作为权重 每次取出K=100*6个单词 的索引 也就是单词数字\n",
    "            编码 这就是负采样的规则  按照单词出现频率进行采样 出现频率越高 被采样的次数概率越高\n",
    "            并且是有放回的\n",
    "            \"\"\"\n",
    "            negative_places = list(\n",
    "                set(range(len(self.word_to_idx))) - set(pos_words))\n",
    "            neg_words = torch.multinomial(self.word_freqs[negative_places], K * pos_words.shape[0], True)\n",
    "            # random.sample从指定序列中随机获取指定长度的片断\n",
    "            # negative_context = random.sample(negative_places, int(len(self.second_class_walks[index]) * k))\n",
    "\n",
    "            return center_word, pos_words, neg_words\n",
    "\n",
    "    \"\"\"\n",
    "    创建dataset 和dataloader并且测试 返回一个样本  中心词  中心词附近的正确单词   负采样出来的负样本 \n",
    "    \"\"\"\n",
    "    dataset = WordEmbeddingDataset(word_pairs, word_to_idx, id_to_word, word_freqs, word_counts)\n",
    "    dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    \"\"\"\n",
    "    定义pytorch模型\n",
    "    \"\"\"\n",
    "\n",
    "    class EmbeddingModel(nn.Module):\n",
    "        def __init__(self, vocab_size, embed_size):\n",
    "            # 初始化输入和输出embedding\n",
    "            super(EmbeddingModel, self).__init__()\n",
    "            # 词典大小\n",
    "            self.vocab_size = vocab_size\n",
    "            # 词嵌入维度\n",
    "            self.embed_size = embed_size\n",
    "            # 初始化的权重范围  可以更好的拟合模型\n",
    "            initrange = 0.5 / self.embed_size\n",
    "\n",
    "            # 嵌入层\n",
    "            self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "            # 初始化输出层嵌入层权重\n",
    "            self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "            # 输入嵌入层\n",
    "            self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "            # 初始化输入嵌入层权重\n",
    "            self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        def forward(self, input_labels, pos_labels, neg_labels):\n",
    "            \"\"\"\n",
    "            input_labels:中心词 [batch_size] [1,2,..........128..............]  输入是数字表示\n",
    "            pos_labels:中心词周围context window 出现过的单词[batch_size*window_size*2] [128*6,]\n",
    "            neg_labels: 中心词周围没有出现的单词  负采样过来的单词 [batch_size*window_size*2*K] >>[128*6*100,]\n",
    "\n",
    "            return: loss, [batch_size]\n",
    "\n",
    "            \"\"\"\n",
    "            batch_size = input_labels.size(0)\n",
    "            \"\"\"\n",
    "            #输入嵌入层  把输入单词词典维度嵌入到embed_size维的空间\n",
    "            #数据维度变化[batch_size,onehot_vocab_size]>>>>[batch_size,embed_size]\n",
    "            # 参数权重的维度   [vocab_size,embed_size] 这就是我们输入的词嵌入 词向量 \n",
    "            #每一行就可以表示为这个单词的 embed_size维度的向量\n",
    "            input_embedding= [batch_size,embed_size]\n",
    "            \"\"\"\n",
    "            input_embedding = self.in_embed(input_labels)\n",
    "            \"\"\"\n",
    "            输出层词嵌入层  和上面词嵌入网络层原理一样\n",
    "            #数据维度变化[batch_size,2*c,onehot_vocab_size]>>>>[batch_size,embed_size]\n",
    "            # 参数权重的维度   [vocab_size,embed_size] 这就是我们输入的词嵌入 词向量 \n",
    "            #每一行就可以表示为这个单词的 embed_size维度的向量\n",
    "             pos_embedding=[batch_size,2*C,embed_size] \n",
    "            \"\"\"\n",
    "            pos_embedding = self.out_embed(pos_labels)\n",
    "            \"\"\"\n",
    "            负采样单词 和预测输出单词共享权重变量\n",
    "            #数据维度变化[batch_size,onehot_vocab_size]>>>>[batch_size,embed_size]\n",
    "            # 参数权重的维度   [vocab_size*2*C×K,embed_size] 这就是我们输入的词嵌入 词向量 \n",
    "            #每一行就可以表示为这个单词的 embed_size维度的向量\n",
    "             neg_embedding= [batch_size,2*C*K,embed_size] \n",
    "            \"\"\"\n",
    "            neg_embedding = self.out_embed(neg_labels)\n",
    "\n",
    "            \"\"\"\n",
    "            计算损失函数\n",
    "            这里的目标函数是重点\n",
    "            简单粗暴的理解下目标函数  \n",
    "            这个模型就做了一件事，就是利用input嵌入层得到输入词向量 \n",
    "            利用输出词向量得到中心词周围的词的词向量和负采样得到的 词向量 \n",
    "            》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》》\n",
    "            那么假设中心词跟周围词关系越近，跟负采样的词关系很远。这个假设很关键，是个隐藏的条件 \n",
    "\n",
    "            那么目标函数为：中心词和周围词进行矩阵乘法，值越大越好。中心词与负采样的词矩阵乘法，值越小越好，取负-，也是越大越好 \n",
    "\n",
    "            这个模型非常的简单，只有嵌入层，没有任何的隐藏层，就是一个表示模型    \n",
    "\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            具体的目标函数  会在文章末尾给出\n",
    "            第一步： 正确的  输出词向量 点积  输入词向量  \n",
    "                            pos_embedding=[batch_size,2*C,embed_size]  input_embedding= [batch_size,embed_size]\n",
    "                            化成相同的形状\n",
    "                            pos_embedding=[batch_size,2*C,embed_size]  input_embedding= [batch_size,embed_size,1]\n",
    "                            得到\n",
    "                            log_pos=[batch_size,2*C,1]>>squeeze()>>>[batch_Size,2*C]\n",
    "            第二步     负采样的  输出词向量 点积  负的 -输入词向量  \n",
    "                            neg_embedding=[batch_size,2*C×K,embed_size]  input_embedding= [batch_size,embed_size]\n",
    "                            化成相同的形状\n",
    "                            pos_embedding=[batch_size,2*C*K,embed_size]  input_embedding= [batch_size,embed_size,1]\n",
    "                            得到\n",
    "                            log_neg=[batch_size,2*C*K,1]>>squeeze()>>>[batch_Size,2*C*K]\n",
    "\n",
    "            第三步： 将log_pos 进行logsigmoid 并在第二维度上进行求和\n",
    "            第四步：  log_neg 进行logsigmoid 并在第二维度上进行求和\n",
    "\n",
    "            第五步   将2者相加 [batch_Size]\n",
    "            \"\"\"\n",
    "            # 计算的是与周围词的相似度   越大越好\n",
    "            log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze()\n",
    "            # 计算的是负采样的词的相似度  取-  越大越好\n",
    "            log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze()\n",
    "\n",
    "            log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "            log_neg = F.logsigmoid(log_neg).sum(1)  # batch_size\n",
    "\n",
    "            loss = log_pos + log_neg\n",
    "\n",
    "            # 取-  那么需要这个值最小\n",
    "            return -loss\n",
    "\n",
    "        def input_embeddings(self):\n",
    "            return self.in_embed.weight.data.cpu().numpy()\n",
    "\n",
    "    # 定义一个模型，以及如果有gpu，就把模型移动到gpu进行计算\n",
    "\n",
    "    model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    训练模型：\n",
    "    模型一般需要训练若干个epoch\n",
    "    每个epoch我们都把所有的数据分成若干个batch\n",
    "    把每个batch的输入和输出都包装成cuda tensor\n",
    "    forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "    用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "    清空模型当前gradient\n",
    "    backward pass\n",
    "    更新模型参数\n",
    "    每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "\n",
    "            # todo\n",
    "            input_labels = input_labels.long()\n",
    "            pos_labels = pos_labels.long()\n",
    "            neg_labels = neg_labels.long()\n",
    "\n",
    "            if USE_CUDA:\n",
    "                nput_labels = input_labels.cuda()\n",
    "                pos_labels = pos_labels.cuda()\n",
    "                neg_labels = neg_labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # loss越小  模型越准确\n",
    "            loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                with open(LOG_FILE, \"a\") as fout:\n",
    "                    fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                    print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "\n",
    "    # 所有模型训练完保存一次\n",
    "    embedding_weights = model.input_embeddings()\n",
    "    state = {\n",
    "        'embedding': embedding_weights,\n",
    "    }  # 建立一个字典，保存训练好的embedding\n",
    "    torch.save(state, '/home/mw/project/embedding_k_is_{}_epoch_{}.pth'.format(k, NUM_EPOCHS))  # 保存最后一轮的参数\n",
    "\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b5ccf",
   "metadata": {
    "id": "C7D8CC6788BD4E89B4859AE75623E694",
    "jupyter": {},
    "mdEditEnable": true,
    "notebookId": "62d909318d38c00d5dfdf15f",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.3 训练poi向量\n",
    "定义好Skip-gram函数后，接下来就可以正式开始训练兴趣点向量了。训练的思路是：\n",
    "\n",
    "1. 首先加载兴趣点数据和深圳市的地块数据，兴趣点数据中包含深圳市2013年和2020年的百度地图兴趣点，深圳市地块数据包含了深圳市每个地块所属的具体编号，深圳市一共有6912个地块，“ZoneID”值为-1代表该poi点不属于任何地块，即不在任何地块范围内。\n",
    "\n",
    "2. 找出有poi的地块（poi_integrate_id.csv文件中不为-1的所有其它“ZoneID”值），逐地块进行循环。对于每个地块，先找出其包含的poi，再针对每个poi，求与其距离最近的k个poi。若地块内包含的poi个数N=1，则与其最近的poi均用此poi代替。若1 < N <= k，则不足的k-N个poi用第一个最近的poi代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22e97df",
   "metadata": {
    "collapsed": false,
    "id": "16BEDEF83DF24B65ABB0E52615CECDCB",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前年份的poi数据词典长度为： 163\n",
      "已构造完2013年的第4125个地块的点对epoch: 0, iter: 0, loss: 700.0784912109375\n",
      "epoch: 0, iter: 100, loss: 55.6304931640625\n",
      "epoch: 0, iter: 200, loss: 54.98585510253906\n",
      "epoch: 0, iter: 300, loss: 54.20474624633789\n",
      "epoch: 0, iter: 400, loss: 53.69351577758789\n",
      "epoch: 0, iter: 500, loss: 53.487510681152344\n",
      "epoch: 0, iter: 600, loss: 54.00068283081055\n",
      "epoch: 0, iter: 700, loss: 54.00804138183594\n",
      "epoch: 0, iter: 800, loss: 52.970821380615234\n",
      "epoch: 0, iter: 900, loss: 53.9283447265625\n",
      "epoch: 0, iter: 1000, loss: 53.66712188720703\n",
      "epoch: 0, iter: 1100, loss: 52.68824005126953\n",
      "epoch: 0, iter: 1200, loss: 51.99803161621094\n",
      "epoch: 1, iter: 0, loss: 52.846492767333984\n",
      "epoch: 1, iter: 100, loss: 53.07270812988281\n",
      "epoch: 1, iter: 200, loss: 53.120826721191406\n",
      "epoch: 1, iter: 300, loss: 52.952796936035156\n",
      "epoch: 1, iter: 400, loss: 53.0266227722168\n",
      "epoch: 1, iter: 500, loss: 52.60991287231445\n",
      "epoch: 1, iter: 600, loss: 53.201385498046875\n",
      "epoch: 1, iter: 700, loss: 52.77553939819336\n",
      "epoch: 1, iter: 800, loss: 53.25457000732422\n",
      "epoch: 1, iter: 900, loss: 53.11608123779297\n",
      "epoch: 1, iter: 1000, loss: 52.70623779296875\n",
      "epoch: 1, iter: 1100, loss: 52.412513732910156\n",
      "epoch: 1, iter: 1200, loss: 52.75755310058594\n",
      "epoch: 2, iter: 0, loss: 53.04802322387695\n",
      "epoch: 2, iter: 100, loss: 52.86192321777344\n",
      "epoch: 2, iter: 200, loss: 51.989990234375\n",
      "epoch: 2, iter: 300, loss: 52.618995666503906\n",
      "epoch: 2, iter: 400, loss: 52.84297180175781\n",
      "epoch: 2, iter: 500, loss: 53.00501251220703\n",
      "epoch: 2, iter: 600, loss: 52.92319107055664\n",
      "epoch: 2, iter: 700, loss: 52.01858139038086\n",
      "epoch: 2, iter: 800, loss: 53.606597900390625\n",
      "epoch: 2, iter: 900, loss: 53.08503341674805\n",
      "epoch: 2, iter: 1000, loss: 52.23661804199219\n",
      "epoch: 2, iter: 1100, loss: 52.68571853637695\n",
      "epoch: 2, iter: 1200, loss: 52.881229400634766\n",
      "词向量训练完成\n"
     ]
    }
   ],
   "source": [
    "integrate_path = u'/home/mw/input/Data5799/数据集/poi_integrate_id.csv'  # 2013与2020年集成poi的文件存储路径\n",
    "pth_parcel = u'/home/mw/input/Data5799/数据集/shenzhen_parcel.csv'  # 地块文件路径\n",
    "Years = [2013, 2020]  # poi数据的年份\n",
    "\n",
    "# 用于加载数据，path为文件路径\n",
    "def load_data(path):\n",
    "    original_poi = pd.read_csv(path, encoding='gb18030')  # 读取原始poi数据\n",
    "    return original_poi\n",
    "\n",
    "\n",
    "# 如果要进行距离计算，需要先进行投影，因为经纬度是球面坐标，不方便直接计算距离，需要先将其转换为平面坐标才方便计算距离。\n",
    "# 该函数用于将wgs84地理坐标转换为对应投影的平面直角坐标\n",
    "# wgs_x为经度坐标，wgs_y为纬度坐标\n",
    "def transform(wgs_x, wgs_y):\n",
    "    p1 = CRS.from_epsg(4326)  # 定义数据地理坐标系，wgs_84地理坐标系的EPSG Code\n",
    "    p2 = CRS.from_epsg(32649)  # WGS_1984_UTM_Zone_49N投影坐标系的EPSG Code\n",
    "    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:32649\")  # 构造转换对象\n",
    "    x, y = transformer.transform(np.array(wgs_y), np.array(wgs_x))  # 参数为元组形式，不能为list。x，y为投影坐标\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "parcel = load_data(pth_parcel)\n",
    "integrate_corpus = load_data(integrate_path)  # 集成的语料库\n",
    "FirstLevel = integrate_corpus['FirstLevel'].drop_duplicates().values.tolist()  # 第一大类，用于可视化，drop_duplicates作用是去重\n",
    "\n",
    "zone_all = integrate_corpus.loc[integrate_corpus['ZoneID'] > -1]  # 有poi的地块编号都不为-1，该变量表示满足此条件的所有poi数据\n",
    "\n",
    "k = 10  # 取k个最邻近的poi作为周围词的数量\n",
    "pairs = []  # 初始化包含点对的列表\n",
    "# 开始按年份循环地块\n",
    "for temp_year in Years:\n",
    "    if temp_year == 2013:  # 作业2中需修改此处的2013为2020\n",
    "        temp_year_poi = integrate_corpus.loc[integrate_corpus['Year'] == temp_year]  # 所有当前年份的poi数据\n",
    "        \n",
    "        # 构造字典\n",
    "        sequence = temp_year_poi['SecondLevel'].values.tolist()  # 当前年份所有的poi二级类，有重复\n",
    "        vocab = dict(Counter(sequence))  # 统计每个类别出现的次数，字典形式\n",
    "        idx_to_word = [word for word in vocab.keys()]\n",
    "        word2id = {word: i for i, word in enumerate(idx_to_word)}  # poi词典，每个poi名称对应一个id序号\n",
    "        print('当前年份的poi数据词典长度为：', len(word2id))\n",
    "\n",
    "        poi_with_zone = temp_year_poi.loc[temp_year_poi['ZoneID'] > -1]  # 当前年份落在地块内的poi，即地块编号>-1的poi\n",
    "\n",
    "        temp_year_zone = poi_with_zone['ZoneID'].drop_duplicates().values.tolist()  # 有poi的当前年份的地块列表\n",
    "\n",
    "        count = 0\n",
    "        # 外层循环为地块的顺序\n",
    "        for order in temp_year_zone:  # 设置地块循环是为了方便记录执行到哪些点了，就是提示进行到哪一步了\n",
    "            # 统计并记录每个地块包含的poi\n",
    "            poi_class = poi_with_zone.loc[poi_with_zone['ZoneID'] == order, 'SecondLevel'].values.tolist()  # poi类别\n",
    "            poi_class = np.array(poi_class)  # 将列表转为数组，便于将编号转为类别文字\n",
    "            poi_Lon = poi_with_zone.loc[poi_with_zone['ZoneID'] == order, 'Lon'].values.tolist()  # 经度\n",
    "            poi_Lat = poi_with_zone.loc[poi_with_zone['ZoneID'] == order, 'Lat'].values.tolist()  # 纬度\n",
    "\n",
    "            # 坐标转换，同一年数据可以直接转换\n",
    "            poi_x, poi_y = transform(poi_Lon, poi_Lat)  # wgs84地理坐标转换为平面坐标\n",
    "            poi_xy = np.array(list(zip(poi_x, poi_y)))  # 当前地块所有poi的经纬度数组\n",
    "            N = poi_xy.shape[0]  # 当前地块poi的总个数\n",
    "\n",
    "            '''构造（中心poi，周围poi）点对，每个poi构造10个对'''\n",
    "            # 如果地块只有一个poi，则复制后直接添加\n",
    "            if N == 1:\n",
    "                pairs.append([])  # 创建二维列表，第一个存储中心词，后面的是k个周围词\n",
    "                center = word2id[poi_class[0]]  # center word\n",
    "                context = [center] * k\n",
    "                pairs[-1].append(center)  # 添加中心词\n",
    "                for w in context:\n",
    "                    pairs[-1].append(w)\n",
    "            else:\n",
    "                dist_all = cdist(poi_xy, poi_xy)  # 求每对点间的距离\n",
    "                row, col = np.diag_indices_from(dist_all)\n",
    "                dist_all[row, col] = np.zeros((dist_all.shape[0],))  # 替换对角值为0，自己和自己距离为0\n",
    "\n",
    "                sorted_index = np.argsort(dist_all)  # 对距离矩阵每一行按从小到大排序，返回array\n",
    "                sorted_index = sorted_index[:, 1:]  # 排除第一列0\n",
    "\n",
    "                # 对每一行求点对\n",
    "                for idx in range(sorted_index.shape[0]):  # 每一行索引代表一个中心词id\n",
    "                    pairs.append([])\n",
    "                    center = word2id[poi_class[idx]]  # center word\n",
    "                    pairs[-1].append(center)  # 末尾添加中心词\n",
    "                    if 1 < N <= k:  # 有几个取前几个，并补齐至k个\n",
    "                        for i in range(k - N + 1):\n",
    "                            context = word2id[poi_class[sorted_index[idx, 0]]]  # 以第一个最近的poi补齐\n",
    "                            pairs[-1].append(context)  # 周围词列表，第0个不取\n",
    "                        n = N - 1\n",
    "                    else:  # 取前k个\n",
    "                        n = k\n",
    "                    for i in range(n):\n",
    "                        context = word2id[poi_class[sorted_index[idx, i]]]\n",
    "                        pairs[-1].append(context)  # 周围词列表，第0个不取\n",
    "            count += 1\n",
    "            # \\r 表示将光标的位置回退到本行的开头位置,配合end=''不换行，实现打印新内容时删除旧内容\n",
    "            print('\\r已构造完{0}年的第{1}个地块的点对'.format(temp_year, count), end=\"\")\n",
    "    else:\n",
    "        continue\n",
    "state = {\n",
    "    'pairs': pairs,\n",
    "}  # 建立一个字典，保存训练好的embedding\n",
    "torch.save(state, '/home/mw/project/pairs_k_is_{}.pth'.format(k))  # 保存训练的点对\n",
    "\n",
    "# pairs = state['pairs']  \n",
    "# count = len(pairs)  # 查看兴趣点pairs的数量\n",
    "embedding = skip_gram(vocab, pairs, word2id, idx_to_word, k)  # 返回词向量\n",
    "print('词向量训练完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de33bd",
   "metadata": {
    "id": "D672A6DA22044421924584DCEFE45A4F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2.4 小结\n",
    "本节介绍了关于Word2Vec、Place2Vec等模型的基本知识，并用实例代码演示了深圳市2013年兴趣点的训练过程和方法，最终得到了深圳市2013年的兴趣点向量。\n",
    "\n",
    "本节我们主要学习到了这些知识：\n",
    "1. 词向量、独热编码的基本概念，以及Word2Vec、Place2Vec模型的基本结构；\n",
    "2. poi数据的处理以及利用Word2Vec/Place2Vec模型训练poi向量的基本流程和方法。\n",
    "\n",
    "下一节，我们将学习聚类和可视化的有关知识，以及如何将这些刚训练的兴趣点向量可视化地展示出来。此外，我们还将介绍如何得到地块向量，以及利用地块向量进行聚类得到土地利用分类数据的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a184285",
   "metadata": {
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 3 兴趣点向量的定性精度评价与土地利用分类\n",
    "**本节使用镜像为Python 3.7 TensorFlow 2.6 PyTorch 1.8，使用的计算资源是2核8G CPU资源，Kernel类型为Python3。由于需要导入训练的向量文件（.pth）以及做聚类分析，因此推荐大家使用包含torch库和Scikit-learn库的镜像。但向量的聚类及可视化不涉及gpu的使用，因此大家使用CPU资源就可以了。**\n",
    "\n",
    "上一节只得到了兴趣点向量，但并不知道所训练的兴趣点向量的效果如何。那**如何评价所训练的兴趣点向量的质量效果呢？** 此外，得到了poi向量后，**如何进一步获得地块向量及利用它进行土地利用分类呢？**\n",
    "\n",
    "首先，可以仿照上一节中Word2Vec模型中关于“北京、上海和东京”等词的词向量可视化来定性评价所训练的兴趣点向量。而由于我们的向量数量较多，不方便直接在图中将文字和点同时标注在一起。因此在可视化之前，我们首先对向量进行聚类，这样可视化时**在同一个聚类簇里面的向量距离相近，具有较高的相似性**。再对不同的聚类簇予以不同的颜色显示，就能很直观地看到聚类的效果了。那什么是聚类分析呢？\n",
    "\n",
    "特别注意：\n",
    "1. **由于聚类前需要先计算Silhouette值以确定最佳的聚类数，因此建议大家在做作业时逐个cell依次运行，根据silhouette值调整聚类数。而不要一键运行，一键运行所有的cell在原参数没有改变的情况下可能无法得到最好的结果。**\n",
    "2. **程序运行到visualize函数时会出现FutureWarning警告，这是包的版本问题，不影响程序运行，因此可以忽略这里的警告。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c0681",
   "metadata": {
    "id": "B14D2731CD2A4FB1B66D5D6B6BE1CC31",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3.1 K-Means聚类算法简介\n",
    "聚类分析就是将大量数据中具有\"相似\"特征的数据点或样本划分为一个类别，聚类分析提供了样本集在非监督模式下的类别划分。聚类的基本思想是\"物以类聚、人以群分\"，将大量数据集中相似的数据样本区分出来，并发现不同类的特征。聚类模型可以建立在无类标记的数据上，是一种非监督的学习算法。聚类根据数据自身的距离或相似度将他们划分为若干组，就是给样本打上不同类的标签，划分原则是组内样本最小化而组间距离最大化。常用的聚类算法有K-Means聚类算法、DBSCAN（具有噪声的密度聚类算法）等，常见的聚类算法及聚类效果可见下图：\n",
    "\n",
    "![Image Name](./pic/rgja58cmn0.png)\n",
    "\n",
    "本例中我们选择K-Means聚类算法进行聚类。那什么是K-Means聚类算法呢？\n",
    "\n",
    "简而言之，K-Means聚类的基本思想就是：对于给定的样本集，按照样本之间的距离(也就是相似程度)大小，将样本集划分为K个簇(即类别)。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。\n",
    "\n",
    "算法步骤：\n",
    "步骤1：随机取k个初始中心点\n",
    "步骤2：对于每个样本点计算到这k个中心点的距离，将样本点归到与之距离最小的那个中心点的簇。这样每个样本都有自己的簇了\n",
    "步骤3：对于每个簇，根据里面的所有样本点重新计算得到一个新的中心点，如果中心点发生变化回到步骤2，未发生变化转到步骤4\n",
    "步骤4：得出结果\n",
    "\n",
    "就像这样：\n",
    "\n",
    "![Image Name](./pic/rgjamlns9r.gif)\n",
    "\n",
    "知道了K-Means聚类的基本原理，那该如何实现它呢？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d0b126",
   "metadata": {
    "collapsed": false,
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "624ef709c47d0200184a3cb9",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  # 导入聚类所需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c2d003a",
   "metadata": {
    "collapsed": false,
    "id": "4FC61B519BE24DD590C9913A15FBDA95",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# K-means聚类,k为聚类数，X为向量数据\n",
    "# 使用欧氏距离作为距离度量标准\n",
    "# 返回聚类结果\n",
    "def kmeans(k, X):\n",
    "    # 正式定义模型\n",
    "    # model1 = KMeans(n_clusters=k, n_init=10)  # n_init：用不同的初始化质心运行算法的次数。\n",
    "    model1 = KMeans(n_clusters=k, random_state=500)  # random_state：用于初始化质心的生成器（generator）,设为固定值保证结果可复现。\n",
    "    # 跑模型\n",
    "    model1.fit(X)\n",
    "    # 需要知道每个类别有哪些参数\n",
    "    cluster_result = model1.predict(X)  # 每个向量所属的类别,[1,1,2,0,3...]这种形式，类别从0开始\n",
    "\n",
    "    return cluster_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c22621",
   "metadata": {
    "id": "16BEDEF83DF24B65ABB0E52615CECDCB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3.2 确定聚类数k\n",
    "通过上述程序可以看到，K-Means聚类的聚类数k是一个超参数，是需要事先给定的。那么该如何确定聚类数k的值呢？\n",
    "\n",
    "这里我们选择利用Silhouette值（轮廓系数）来决定k值。Silhouette值是一个聚类的评价指标，用来描述一个目标对于目标所在簇与其他簇之间的相似性。其范围是从-1~+1，这个值越大表明目标与自己所在簇之间的匹配关系度越高，与其他簇的匹配关系度越低。**如果这个值越高，那么聚类结果越好**，如果是很小或是负值，那么可能是分簇太多或是太少造成的。\n",
    "\n",
    "因此，为了确定具体的k值，我们选择将k值取[2,26]之间的整数，分别比较取各个k时的轮廓系数，选择其中轮廓系数相对较高的k值作为我们最终的聚类数。具体的计算程序及轮廓系数可视化的程序如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d68781c7",
   "metadata": {
    "collapsed": false,
    "id": "640A2933511548669347156F0E31B601",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 先导入计算轮廓系数相关的包和绘图可视化包\n",
    "from sklearn.metrics import silhouette_score  # score计算所有样本的平均轮廓系数\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26ba9926",
   "metadata": {
    "collapsed": false,
    "id": "D672A6DA22044421924584DCEFE45A4F",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算各个分类的平均轮廓系数(silhouette值),X为向量,m为不同的聚类数2~m构成一个聚类区间\n",
    "# m的作用相当于对k分别取值2~m之间的整数进行聚类，找出其中Silhouette值较大的一个k值\n",
    "def compute_silhouette(X, m):  \n",
    "    # 依次对不同的k值进行聚类,k取值为2~m之间的整数（不包括m）\n",
    "    # n_int指定了k均值算法运行的次数。每一次都会选择一组不同的初始化均值向量，最终算法会选择最佳的分类簇来作为最终的结果。\n",
    "    kmeans_per_k = [KMeans(n_clusters=k, random_state=500).fit(X) for k in range(1, m)]  \n",
    "    # 必须至少有两类才能计算silhouette值，所以从1开始\n",
    "    silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]\n",
    "\n",
    "    # 画图\n",
    "    # plt.figure(figsize=(8, 3))  # figure的宽和高，单位为英寸\n",
    "    plt.plot(list(range(2, m)), silhouette_scores, color='red', marker='o')\n",
    "    plt.xlabel(\"k\", fontdict={'size': 16})\n",
    "    plt.ylabel(\"Silhouette score\", fontdict={'size': 16})\n",
    "    plt.xticks(range(2, m, 1))  # 设置x轴坐标间隔\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4022b5e",
   "metadata": {
    "id": "A554BCE36EC9476C84EDDBF02CAE4284",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3.3 向量降维及可视化\n",
    "确定了聚类数k以及利用k值进行聚类后，由于向量的维度较高，达到256维，因此不能直接在二维平面进行显示，需要先将其降至2维才方便显示。那么该如何对向量进行降维呢？\n",
    "\n",
    "常用的降维方法有PCA（主成分分析）、T-SNE（t-分布领域嵌入算法）方法等。这里我们选择T-SNE方法进行降维。它的主要想法就是：将高维分布点的距离，用条件概率来表示相似性，同时低维分布的点也这样表示。只要二者的条件概率非常接近（用相对熵来训练，所以需要label），那就说明高维分布的点已经映射到低维分布上了。\n",
    "\n",
    "具体的数学原理这里不做过多介绍，感兴趣的同学可以下去具体了解。这里主要介绍利用T-SNE方法对向量进行降维及向量降维后的可视化方法，具体程序如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b8e5a4b",
   "metadata": {
    "collapsed": false,
    "id": "93D30EAD8B0648E0BB08F272A5A92114",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE  # 导入T-SNE方法的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3baaa098",
   "metadata": {
    "collapsed": false,
    "id": "7E7495DCD69A43C98A7227DB99290727",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义向量降维可视化的函数\n",
    "# embedding为需要可视化的向量，category为聚类类别\n",
    "def visualize(embedding, category):\n",
    "    \"\"\"T-SNE降维\"\"\"\n",
    "    # 对原始向量降维\n",
    "    X_tsne = TSNE(n_components=2, init='pca', random_state=33).fit_transform(embedding)  # 降至二维\n",
    "    x_min, x_max = X_tsne.min(0), X_tsne.max(0)  # min(0)返回该矩阵中每一列的最小值，max(0)返回该矩阵中每一列的最大值\n",
    "    X_norm = (X_tsne - x_min) / (x_max - x_min)\n",
    "\n",
    "    # 计算质心坐标\n",
    "    centroids_xy = []  # 初始化每一类簇的质心坐标\n",
    "    for temp_class in range(np.max(category) + 1):\n",
    "        temp_poi_id = np.argwhere(category == temp_class)  # 为二维数组，shape只有一列\n",
    "        temp_poi_id = temp_poi_id.flatten()  # 展平为一维数组\n",
    "        temp_class_embedding = X_norm[temp_poi_id.tolist()]  # 找出当前类的poi嵌入向量，二维数组\n",
    "        temp_centroid_xy = np.mean(temp_class_embedding, axis=0)  # 找出当前类点的质心坐标，一维数组\n",
    "        centroids_xy.extend([temp_centroid_xy.tolist()])\n",
    "\n",
    "    # 画图\n",
    "    plt.figure(figsize=(12, 12))  # 设置画布大小,可以根据需要调整\n",
    "    plt.scatter(X_norm[:, 0], X_norm[:, 1], c=category, cmap=plt.cm.rainbow)\n",
    "\n",
    "    # 添加注释\n",
    "    cluster_id = ['C' + str(i) for i in range(np.max(category) + 1)]  # 聚类名列表，用于在每个类簇的质心指示具体的类簇是哪个\n",
    "    for i in range(len(cluster_id)):\n",
    "        plt.text(centroids_xy[i][0], centroids_xy[i][1], cluster_id[i], fontsize=16,\n",
    "                    color=\"black\",\n",
    "                    style=\"normal\",\n",
    "                    weight=\"bold\", verticalalignment='center',\n",
    "                    horizontalalignment='right')\n",
    "\n",
    "    # 设置坐标刻度值的大小\n",
    "    plt.yticks(size=16)\n",
    "    plt.xticks(size=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ac99a",
   "metadata": {
    "id": "043ED93C5BEF4CCB910CE40C8B8584CA",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3.4 poi向量聚类可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3154aa8f",
   "metadata": {
    "collapsed": false,
    "id": "6E84C5AA351F40109EC0100DA9C28F0D",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义好聚类及可视化的函数以后，接下来就可以开始主程序的撰写了\n",
    "# 先导入相关包\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd71dfae",
   "metadata": {
    "collapsed": false,
    "id": "2B5FFEEF4CCC44938200F83E7C7C88D2",
    "jupyter": {
     "outputs_hidden": false
    },
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/2B5FFEEF4CCC44938200F83E7C7C88D2/rgnm2zbjc3.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "place2vec_path='/home/mw/project/embedding_k_is_10_epoch_3.pth'  # 上一节训练的向量的保存路径\n",
    "\n",
    "# 最大最小值归一化处理，便于绘图显示，规范坐标\n",
    "def normalization(X):\n",
    "    # 特征缩放\n",
    "    X_min, X_max = np.min(X), np.max(X)\n",
    "    X = (X - X_min) / (X_max - X_min)\n",
    "    # 根据数据需要进行不同的处理\n",
    "    return X\n",
    "\n",
    "# 获取向量\n",
    "state = torch.load(place2vec_path)\n",
    "poi_embedding = state['embedding']  # place2vec方法得到的原始向量\n",
    "poi_embedding = normalization(poi_embedding)  # 最大最小值归一化\n",
    "k1 = 26  # 初始向量聚类数k的试验区间为[2,26],k为整数\n",
    "compute_silhouette(poi_embedding, k1)  # 计算每个k值下的平均silhouette值，确定最优k值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cdecc",
   "metadata": {
    "id": "8D4D6B42FE4F4035BB797D4006A2E488",
    "jupyter": {},
    "mdEditEnable": true,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如上图可见，选取平均Silhouette值曲线图中较大Silhouette值对应的k值作为下面聚类的聚类数，即18。因此，如下方程序所示，kmeans函数的第一个参数设置为18。\n",
    "\n",
    "**特别注意，在做作业时这里需要大家根据具体的数据以及具体的平均Silhouette值曲线图选择合适的聚类数k，因为2013年和2020年的poi向量存在差异，它们的最佳聚类数也不一定相同。**\n",
    "\n",
    "确定了聚类数以后便可以继续下面对于poi向量的kmeans聚类以及可视化了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b27fcf5",
   "metadata": {
    "collapsed": false,
    "id": "B0015847F9DF40A49910E5CA0863C2FA",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  2  2  2  0  2  0  2  0 15  0  0  2  0  9 13  5 13 15  9  5  0  0  9\n",
      "  0  0  9  0  0  0  0  4  4  2  4  0  0 14 14  9 10  0 16  4  0  0  0  0\n",
      "  0  0  0  3 12  5 12  0  0 12  0  0  0  0  0  0 15  0  0  5  5  9  0  0\n",
      " 13 13 13  8 13  0 14  0 15  6 15  6 15  6  0  0  0  0  0  0  0  0  0  0\n",
      " 16 16 11 16 16  0  7 17  7  0  3  0 17  9 16  3  0  7  7  7  9  0  0  0\n",
      "  8  0  0  0  0 17  3  1  3 16 17 17  3  1  1  1  1 17 17 17 17  0  0  2\n",
      "  2  0  0  0  0  9 10 10 14 14  0  0  7  7  9  9  9  9  0]\n",
      "(163,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/B0015847F9DF40A49910E5CA0863C2FA/rgnm396vk5.png\">"
      ],
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_category = kmeans(18, poi_embedding)  # 使用K-Means聚类\n",
    "print(cluster_category)  # 查看聚类结果\n",
    "print(cluster_category.shape)  # 查看向量维度\n",
    "visualize(poi_embedding, cluster_category)  # 可视化poi聚类向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63239251",
   "metadata": {
    "id": "AFC9846527F344CF8A87A970E397C9B7",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "上图可以根据颜色明显看出类簇呈现相对聚集的分布，向量化效果还不错。**定性评价向量化效果没有特别固定的标准，若相同颜色的点聚在一起地越紧凑，并与不同颜色的类簇之间有明显的区别，则聚类效果越好，向量化的效果也越好。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49bcf4",
   "metadata": {
    "id": "89BC6A9B6A54408F972C5E099C15023E",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3.5 地块向量的获取及聚类可视化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f176e66",
   "metadata": {
    "collapsed": false,
    "id": "96B21517344141439F2898A949540DD6",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 获取d年份地块向量，最终得到每个地块的变化向量\n",
    "# embedding为通过Word2Vec模型获取的poi嵌入向量\n",
    "def parcel_embedding(embedding, integrated_poi, year):\n",
    "    temp_year_poi = integrated_poi.loc[integrated_poi['Year'] == year]  \n",
    "    zone = temp_year_poi.loc[\n",
    "        integrated_poi['ZoneID'] > -1, 'ZoneID'].drop_duplicates().values.tolist()  # 有poi的地块编号都不为-1\n",
    "\n",
    "    # 构造字典\n",
    "    sequence = temp_year_poi['SecondLevel'].values.tolist()  # 当前年份所有的poi二级类，有重复\n",
    "    vocab = dict(Counter(sequence))  # 统计每个类别出现的次数，字典形式\n",
    "    idx_to_word = [word for word in vocab.keys()]\n",
    "    word2id = {word: i for i, word in enumerate(idx_to_word)}  # poi词典，每个poi名称对应一个id序号\n",
    "\n",
    "    count = 0\n",
    "    parcel_mean = np.zeros(shape=(0, embedding.shape[1]))  # 初始化地块向量的空数组，用于拼接\n",
    "    parcel_contain_all_poi = []  # 初始化包含poi的地块列表\n",
    "    # 外层循环为地块的顺序\n",
    "    for order in zone:\n",
    "        parcel_contain_all_poi.append(order)  # 添加当前地块编号\n",
    "        # 统计并记录每个地块包含的poi\n",
    "        class_temp_year = temp_year_poi.loc[temp_year_poi['ZoneID'] == order, 'SecondLevel'].values.tolist()  # 当前地块2013年poi类别\n",
    "        id_temp_year = [word2id[level] for level in class_temp_year]  # 根据poi类别获取其在词典中的id\n",
    "\n",
    "        '''求地块平均向量'''\n",
    "        # 首先获取向量\n",
    "        embedding_temp_year = embedding[id_temp_year]\n",
    "        # axis=0表示对每行求平均\n",
    "        mean_temp_year = embedding_temp_year.mean(axis=0)\n",
    "        # 向量拼接,需要对embedding_change扩充一个维度才能拼接\n",
    "        parcel_mean = np.concatenate((parcel_mean, mean_temp_year[np.newaxis, :]), axis=0)  # 纵向拼接\n",
    "\n",
    "        count += 1\n",
    "        # \\r 表示将光标的位置回退到本行的开头位置,配合end=''不换行，实现打印新内容时删除旧内容\n",
    "        print('\\r已处理完第{}个地块'.format(count), end=\"\") \n",
    "\n",
    "    return parcel_mean, parcel_contain_all_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69478b57",
   "metadata": {
    "collapsed": false,
    "id": "6F980A57C6A0409D80C2D45439E34B25",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理完第4125个地块"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/6F980A57C6A0409D80C2D45439E34B25/rgnm84c9ij.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "integrate_path = u'/home/mw/input/Data5799/数据集/poi_integrate_id.csv'  # 2013与2020年集成poi的文件存储路径\n",
    "original_poi = pd.read_csv(integrate_path, encoding='gb18030')  # 读取原始poi数据\n",
    "k2 = 16  # 地块向量聚类数k的试验区间为[2,26],k为整数\n",
    "\n",
    "current_year = 2013 # 2013年poi, 作业2中需修改此处2013为2020 \n",
    "# parcel_embed为地块向量，parcel_contain_pois为包含当前年份poi的地块\n",
    "parcel_embed, parcel_contain_pois = parcel_embedding(poi_embedding, original_poi, current_year)\n",
    "compute_silhouette(parcel_embed, k2)  # 计算每个k值下的平均silhouette值，确定最优k值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae4757",
   "metadata": {
    "id": "8AB3FD1967BC47DA88BB27117D933699",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "这里也是一样，选取平均Silhouette值曲线图中较大Silhouette值对应的k值作为下面聚类的聚类数，即令parcel_k=5。\n",
    "\n",
    "确定了聚类数以后便可以继续下面的地块向量的kmeans聚类以及聚类结果分布映射可视化了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "526989ae",
   "metadata": {
    "collapsed": false,
    "id": "FAF7FF3D53884AC782FCFBCD36CB7067",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parcel_k = 5\n",
    "parcel_cluster_category = kmeans(parcel_k, parcel_embed)  # 使用K-Means聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c1c4d87",
   "metadata": {
    "collapsed": false,
    "id": "89F68CF0CA1B45BC9AC10A8C728655F1",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将地块向量差的聚类结果、以及两期地块向量的夹角余弦距离写入csv\n",
    "# change_result为地块变化向量的聚类结果，cosine_dist为两期地块的余弦距离,parcel_contains_all_poi代表两期poi都包含的地块\n",
    "# ZoneCluster_change_pth为变化向量代表的地块编号及该地块所属的聚类结果的存储路径\n",
    "def save_result(parcel_path, save_pth, change_result, parcel_contains_all_poi):\n",
    "    parcel = pd.read_csv(parcel_path, encoding='gb18030')  # 加载地块数据\n",
    "\n",
    "    '''新建列'''\n",
    "    cluster_num = np.max(change_result) + 1  # 聚类数量，因为从0开始，所以要+1\n",
    "    parcel['Cluster'] = [cluster_num] * parcel.shape[0]  # 以最后一类作为不包含poi地块的分类，初始化这一列时数量等于parcel的行数量\n",
    "\n",
    "    count = 0\n",
    "    # 外层循环为地块的顺序,对有poi的地块重新赋值\n",
    "    for order in parcel_contains_all_poi:\n",
    "        parcel.loc[parcel['FID'] == order, 'Cluster'] = change_result[count]  # 写入变化向量的聚类结果\n",
    "        count += 1\n",
    "        # \\r 表示将光标的位置回退到本行的开头位置,配合end=''不换行，实现打印新内容时删除旧内容\n",
    "        print('\\r已处理完第{}个地块'.format(count), end=\"\")\n",
    "\n",
    "    parcel.drop(columns=['城市'], inplace=True)  # 删除城市列, 便于arcgis属性连接\n",
    "\n",
    "    # 输出\n",
    "    parcel.to_csv(save_pth, index=False, encoding='gb18030')  # 设置index=False是为了不输出unnamed列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d68780fb",
   "metadata": {
    "collapsed": false,
    "id": "D39F274B25064115825BE4D69DC9A2E5",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理完第4125个地块\n",
      "地块向量聚类结果输出完成！\n"
     ]
    }
   ],
   "source": [
    "parcel_pth = u'/home/mw/input/Data5799/数据集/shenzhen_parcel.csv'  # 地块文件路径\n",
    "save_pth = '/home/mw/project/parcel_cluster_result_{}.csv'.format(current_year)\n",
    "save_result(parcel_pth, save_pth, parcel_cluster_category, parcel_contain_pois)\n",
    "print('\\n地块向量聚类结果输出完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4507832",
   "metadata": {
    "id": "909845F5EF6A463D83B937AB3D2A2FB2",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 土地利用分类效果展示\n",
    "利用输出的csv文件，结合ArcGIS或者QGIS软件，将地块向量的聚类结果映射到深圳市地图中。最终的效果图是这样：\n",
    "\n",
    "![Image Name](./pic/rgl5hcpmov.jpeg)\n",
    "\n",
    "其中不同的颜色代表不同的类别，灰色的地块代表的是没有poi存在的地块。\n",
    "\n",
    "**特别注意：此处涉及专业软件使用，不要求大家出图，只需要知道如何聚类以及如何得到地块聚类结果和对其进行输出就可以了。如果感兴趣大家可以下去进一步研究利用ArcGIS或者QGIS软件出图~**\n",
    "\n",
    "我们这里以QGIS为例，提供一些相关的学习参考资料：\n",
    "1. [QGIS简介与安装](https://sharuxue.blog.csdn.net/article/details/121191811?spm=1001.2014.3001.5502)\n",
    "2. [QGIS属性表连接及渲染可视化](https://blog.csdn.net/QGISClass/article/details/108689954)\n",
    "\n",
    "以参考资料2为例（可以参照参考资料2进行操作），这里用QGIS可视化地图的基本操作是：\n",
    "1. **添加矢量地图数据。** 直接将地图文件从文件夹拖动至地图显示框中，在QGIS中加载深圳市底图和行政区划边界（由于数据的原因，不方便公开，大家只需要知道这个操作流程即可~）；\n",
    "\n",
    "\n",
    "![Image Name](./pic/rgn2t2f1x.png)\n",
    "\n",
    "2. **添加聚类结果属性表数据。** 点击【打开数据源管理器】按钮，切换到【分隔文本文件】标签页，浏览到我们最后生成的parcel_cluster_result_2013.csv或者parcel_cluster_result_2020.csv这两个文件，编码方式选择“GB18030”，自定义设置图层名称，添加图层；\n",
    "\n",
    "![Image Name](./pic/rgn2ol5r9t.png)\n",
    "\n",
    "3.  **进行属性表连接。** 点击菜单【地理处理】->【工具箱】，打开处理工具箱面板，在【处理工具箱】的搜索栏中输入“join”，找到【矢量通用】->【按字段值连接属性】，双击运行该工具，第一个图层为深圳市地图图层，第二个图层为我们添加的属性表结果图层。两个图层的连接字段均为\"FID\"。\n",
    "\n",
    "\n",
    "![Image Name](./pic/rgn2yox55u.png)\n",
    "\n",
    "4. **可视化地图图层。** 点击【图层】面板的【打开图层样式】按钮，在右侧打开图层样式面板。点击【图层样式】面板渲染器下拉框，选择“分类”渲染。分类是根据图层中指定属性字段取值设置符号，字段的每个取值对应一个符号。该渲染方式特别适合于表达属性字段为字符型、包含对图层要素的分类信息的数据，例如我们这里的例子，将不同的地块划分为不同的土地利用类别。然后点击【值】下拉框，选择“Total_population”（年末总人口）字段，点击下方的【分类】按钮，一幅默认的专题图显示在地图窗口中。\n",
    "\n",
    "![Image Name](./pic/rgn30sh4n2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c032fe",
   "metadata": {
    "id": "F0A316CF774140A596230352FABD64C2",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "62f7109d7384122463646205",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3.6 小结\n",
    "本节我们学习了K-Means聚类算法的原理及实现以及向量降维可视化的方法，实现了兴趣点向量的定性精度评价。\n",
    "\n",
    "本节我们主要学习到了这些知识：\n",
    "1. 聚类的相关概念、K-Means聚类算法的原理、确定聚类数k的方法（利用轮廓系数判定）以及T-SNE降维可视化的概念和方法；\n",
    "2. 利用兴趣点向量得到地块向量的方法（取地块内所有兴趣点向量的平均值）以及利用地块向量进行聚类得到土地利用分类数据的方法。\n",
    "\n",
    "截至到目前，我们的项目《基于兴趣点数据的土地利用分类：以深圳市为例》的教学内容就全部结束啦~ 希望大家能认真研读，通过本项目的学习，**基本掌握有关兴趣点的爬取、训练以及聚类可视化这一套完整的流程的相关基础知识**，并有所收获。为了让大家进一步巩固学习内容，我们还在下一节给大家设置了两个小作业，相信大家已经迫不及待去动手实践了，一起去完成吧~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
